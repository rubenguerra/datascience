{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxgt+WjCFDU7/yxIGzZpNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenguerra/datascience/blob/master/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsxrwTlFzEQD"
      },
      "source": [
        "#Usando una red GAN para generar imÃ¡genes realistas desde el Cifar10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prSnSesajez3"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from numpy import zeros, ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import  Sequential\n",
        "from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8e2JITpkvwp"
      },
      "source": [
        "#Discriminador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWOfKvejkpP4"
      },
      "source": [
        "def define_discriminator(in_shape=(32, 32, 3)):\n",
        "  model=Sequential()\n",
        "  model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(128,(3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTo3Mn0KnZOh"
      },
      "source": [
        "def define_generator(latent_dim):\n",
        "  model=Sequential()\n",
        "  n_nodes=256*4*4\n",
        "  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((4,4,256)))\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nvuN-dmph0C"
      },
      "source": [
        "#Definimos la Red GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnJFITyapeIJ"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "  d_model.trainable = False\n",
        "  model=Sequential()\n",
        "  model.add(g_model)\n",
        "  model.add(d_model)\n",
        "  opt=Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYN6WqeRqWQ2"
      },
      "source": [
        "#Configuracion de datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAE-Ytg8qTm2"
      },
      "source": [
        "def load_real_samples():\n",
        "  (trainX, _), (_,_) = load_data()\n",
        "  X=trainX.astype('float32')\n",
        "  X=(X - 127.5) / 127.5\n",
        "  return X\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  ix=randint(0, dataset.shape[0], n_samples)\n",
        "  X = dataset[ix]\n",
        "  y = ones((n_samples, 1))\n",
        "  return X, y\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  x_input=randn(latent_dim * n_samples)\n",
        "  x_input=x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "  x_input=generate_latent_points(latent_dim, n_samples)\n",
        "  X= g_model.predict(x_input)\n",
        "  y = zeros((n_samples, 1))\n",
        "  return X, y\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Q03lXzssaF"
      },
      "source": [
        "#Entrenando la neurona GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1RpHnRcsppQ"
      },
      "source": [
        "def train(g_model, d_model,gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
        "  bat_per_epo=int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "      d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "      X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "      X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "    if (i+1) % 10 == 0:\n",
        "      summarize_performance(i, g_model, d_model,dataset, latent_dim)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGR5D_W6vqtS"
      },
      "source": [
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "  X_real, y_real=generate_real_samples(dataset, n_samples)\n",
        "  _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "  x_fake, y_fake=generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "  print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "  save_plot(x_fake, epoch)\n",
        "  filename='generator_model_%03d.h5' % (epoch+1)\n",
        "  g_model.save(filename)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cuaiZ6txpcj"
      },
      "source": [
        "#Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RMP-auYxm_e",
        "outputId": "a241bc8b-06f1-426c-ef19-f5b8981ce565"
      },
      "source": [
        "latent_dim = 100\n",
        "d_model = define_discriminator()\n",
        "g_model = define_generator(latent_dim)\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "dataset=load_real_samples()\n",
        "train(g_model, d_model, gan_model, dataset,latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            ">1, 1/390, d1=0.700, d2=0.696 g=0.691\n",
            ">1, 2/390, d1=0.638, d2=0.698 g=0.690\n",
            ">1, 3/390, d1=0.583, d2=0.702 g=0.686\n",
            ">1, 4/390, d1=0.505, d2=0.711 g=0.678\n",
            ">1, 5/390, d1=0.375, d2=0.729 g=0.667\n",
            ">1, 6/390, d1=0.259, d2=0.752 g=0.662\n",
            ">1, 7/390, d1=0.180, d2=0.761 g=0.678\n",
            ">1, 8/390, d1=0.098, d2=0.730 g=0.719\n",
            ">1, 9/390, d1=0.075, d2=0.696 g=0.766\n",
            ">1, 10/390, d1=0.052, d2=0.693 g=0.803\n",
            ">1, 11/390, d1=0.039, d2=0.684 g=0.838\n",
            ">1, 12/390, d1=0.060, d2=0.659 g=0.861\n",
            ">1, 13/390, d1=0.043, d2=0.646 g=0.883\n",
            ">1, 14/390, d1=0.058, d2=0.628 g=0.886\n",
            ">1, 15/390, d1=0.097, d2=0.652 g=0.854\n",
            ">1, 16/390, d1=0.069, d2=0.696 g=0.841\n",
            ">1, 17/390, d1=0.095, d2=0.805 g=0.802\n",
            ">1, 18/390, d1=0.081, d2=0.885 g=0.789\n",
            ">1, 19/390, d1=0.077, d2=0.839 g=0.891\n",
            ">1, 20/390, d1=0.131, d2=0.636 g=1.050\n",
            ">1, 21/390, d1=0.079, d2=0.496 g=1.153\n",
            ">1, 22/390, d1=0.043, d2=0.454 g=1.196\n",
            ">1, 23/390, d1=0.052, d2=0.444 g=1.179\n",
            ">1, 24/390, d1=0.039, d2=0.442 g=1.140\n",
            ">1, 25/390, d1=0.022, d2=0.447 g=1.117\n",
            ">1, 26/390, d1=0.036, d2=0.442 g=1.113\n",
            ">1, 27/390, d1=0.006, d2=0.418 g=1.164\n",
            ">1, 28/390, d1=0.008, d2=0.383 g=1.258\n",
            ">1, 29/390, d1=0.015, d2=0.337 g=1.394\n",
            ">1, 30/390, d1=0.012, d2=0.281 g=1.599\n",
            ">1, 31/390, d1=0.003, d2=0.230 g=1.875\n",
            ">1, 32/390, d1=0.006, d2=0.178 g=2.178\n",
            ">1, 33/390, d1=0.047, d2=0.150 g=2.461\n",
            ">1, 34/390, d1=0.028, d2=0.132 g=2.893\n",
            ">1, 35/390, d1=0.101, d2=0.257 g=3.492\n",
            ">1, 36/390, d1=0.111, d2=0.047 g=3.958\n",
            ">1, 37/390, d1=0.080, d2=0.181 g=5.605\n",
            ">1, 38/390, d1=0.409, d2=0.398 g=3.654\n",
            ">1, 39/390, d1=0.044, d2=0.041 g=4.180\n",
            ">1, 40/390, d1=0.043, d2=0.107 g=5.051\n",
            ">1, 41/390, d1=0.071, d2=0.017 g=4.289\n",
            ">1, 42/390, d1=0.057, d2=0.445 g=10.974\n",
            ">1, 43/390, d1=1.859, d2=0.002 g=5.710\n",
            ">1, 44/390, d1=0.475, d2=0.022 g=3.417\n",
            ">1, 45/390, d1=0.029, d2=0.050 g=2.985\n",
            ">1, 46/390, d1=0.001, d2=0.059 g=2.902\n",
            ">1, 47/390, d1=0.003, d2=0.062 g=2.890\n",
            ">1, 48/390, d1=0.001, d2=0.065 g=2.939\n",
            ">1, 49/390, d1=0.011, d2=0.078 g=2.997\n",
            ">1, 50/390, d1=0.006, d2=0.071 g=3.141\n",
            ">1, 51/390, d1=0.001, d2=0.096 g=3.896\n",
            ">1, 52/390, d1=0.111, d2=0.056 g=4.143\n",
            ">1, 53/390, d1=0.022, d2=0.212 g=5.557\n",
            ">1, 54/390, d1=0.103, d2=0.041 g=5.055\n",
            ">1, 55/390, d1=0.053, d2=5.921 g=3.255\n",
            ">1, 56/390, d1=0.222, d2=5.825 g=3.591\n",
            ">1, 57/390, d1=0.372, d2=1.028 g=8.820\n",
            ">1, 58/390, d1=1.297, d2=0.006 g=5.525\n",
            ">1, 59/390, d1=0.750, d2=0.777 g=3.331\n",
            ">1, 60/390, d1=1.224, d2=0.295 g=2.200\n",
            ">1, 61/390, d1=0.375, d2=0.330 g=2.449\n",
            ">1, 62/390, d1=0.317, d2=0.142 g=2.559\n",
            ">1, 63/390, d1=0.398, d2=0.160 g=2.265\n",
            ">1, 64/390, d1=0.259, d2=0.185 g=2.303\n",
            ">1, 65/390, d1=0.220, d2=0.184 g=2.090\n",
            ">1, 66/390, d1=0.172, d2=0.228 g=2.030\n",
            ">1, 67/390, d1=0.147, d2=0.308 g=2.022\n",
            ">1, 68/390, d1=0.221, d2=1.072 g=1.622\n",
            ">1, 69/390, d1=0.300, d2=3.221 g=1.348\n",
            ">1, 70/390, d1=0.584, d2=0.445 g=3.055\n",
            ">1, 71/390, d1=0.957, d2=0.148 g=2.707\n",
            ">1, 72/390, d1=0.880, d2=0.362 g=2.081\n",
            ">1, 73/390, d1=0.588, d2=0.332 g=2.009\n",
            ">1, 74/390, d1=0.577, d2=0.322 g=2.064\n",
            ">1, 75/390, d1=0.312, d2=0.212 g=2.445\n",
            ">1, 76/390, d1=0.441, d2=0.179 g=2.690\n",
            ">1, 77/390, d1=0.398, d2=0.175 g=2.879\n",
            ">1, 78/390, d1=0.318, d2=0.191 g=3.525\n",
            ">1, 79/390, d1=0.492, d2=0.142 g=3.652\n",
            ">1, 80/390, d1=0.321, d2=0.094 g=4.033\n",
            ">1, 81/390, d1=0.593, d2=0.170 g=4.290\n",
            ">1, 82/390, d1=0.593, d2=0.081 g=3.811\n",
            ">1, 83/390, d1=0.593, d2=0.180 g=3.696\n",
            ">1, 84/390, d1=0.404, d2=0.129 g=3.251\n",
            ">1, 85/390, d1=0.376, d2=0.202 g=2.983\n",
            ">1, 86/390, d1=0.446, d2=0.258 g=2.827\n",
            ">1, 87/390, d1=0.525, d2=0.229 g=2.511\n",
            ">1, 88/390, d1=0.383, d2=0.268 g=2.717\n",
            ">1, 89/390, d1=0.415, d2=0.232 g=2.604\n",
            ">1, 90/390, d1=0.605, d2=0.736 g=1.750\n",
            ">1, 91/390, d1=0.376, d2=1.937 g=1.332\n",
            ">1, 92/390, d1=0.691, d2=1.341 g=3.045\n",
            ">1, 93/390, d1=0.652, d2=0.109 g=3.789\n",
            ">1, 94/390, d1=0.740, d2=0.162 g=3.603\n",
            ">1, 95/390, d1=0.566, d2=0.472 g=4.310\n",
            ">1, 96/390, d1=0.456, d2=0.045 g=4.476\n",
            ">1, 97/390, d1=0.695, d2=0.111 g=3.049\n",
            ">1, 98/390, d1=0.628, d2=0.177 g=2.832\n",
            ">1, 99/390, d1=0.540, d2=0.148 g=3.186\n",
            ">1, 100/390, d1=0.469, d2=0.145 g=3.659\n",
            ">1, 101/390, d1=0.417, d2=0.137 g=3.924\n",
            ">1, 102/390, d1=0.367, d2=0.080 g=3.948\n",
            ">1, 103/390, d1=0.470, d2=0.160 g=4.507\n",
            ">1, 104/390, d1=0.347, d2=0.038 g=4.787\n",
            ">1, 105/390, d1=0.693, d2=0.178 g=4.338\n",
            ">1, 106/390, d1=0.352, d2=0.051 g=4.491\n",
            ">1, 107/390, d1=0.396, d2=0.121 g=4.276\n",
            ">1, 108/390, d1=0.444, d2=0.163 g=4.660\n",
            ">1, 109/390, d1=0.543, d2=0.157 g=4.076\n",
            ">1, 110/390, d1=0.364, d2=0.171 g=4.278\n",
            ">1, 111/390, d1=0.412, d2=0.162 g=4.073\n",
            ">1, 112/390, d1=0.318, d2=0.153 g=3.820\n",
            ">1, 113/390, d1=0.218, d2=0.087 g=3.729\n",
            ">1, 114/390, d1=0.671, d2=0.226 g=3.465\n",
            ">1, 115/390, d1=0.194, d2=0.098 g=3.606\n",
            ">1, 116/390, d1=0.368, d2=0.281 g=4.010\n",
            ">1, 117/390, d1=0.399, d2=0.317 g=4.526\n",
            ">1, 118/390, d1=0.472, d2=0.199 g=4.224\n",
            ">1, 119/390, d1=0.369, d2=0.270 g=3.883\n",
            ">1, 120/390, d1=0.671, d2=1.415 g=4.513\n",
            ">1, 121/390, d1=0.790, d2=0.147 g=4.380\n",
            ">1, 122/390, d1=0.760, d2=1.688 g=3.810\n",
            ">1, 123/390, d1=1.043, d2=0.288 g=3.624\n",
            ">1, 124/390, d1=0.889, d2=0.260 g=3.164\n",
            ">1, 125/390, d1=0.945, d2=0.213 g=2.979\n",
            ">1, 126/390, d1=0.809, d2=0.168 g=2.585\n",
            ">1, 127/390, d1=0.608, d2=0.187 g=2.479\n",
            ">1, 128/390, d1=0.521, d2=0.252 g=2.491\n",
            ">1, 129/390, d1=0.664, d2=0.263 g=2.491\n",
            ">1, 130/390, d1=0.578, d2=0.193 g=2.738\n",
            ">1, 131/390, d1=0.598, d2=0.167 g=2.692\n",
            ">1, 132/390, d1=0.688, d2=0.332 g=2.537\n",
            ">1, 133/390, d1=0.577, d2=0.272 g=2.785\n",
            ">1, 134/390, d1=0.735, d2=0.118 g=2.817\n",
            ">1, 135/390, d1=0.604, d2=0.123 g=2.790\n",
            ">1, 136/390, d1=0.593, d2=0.144 g=2.756\n",
            ">1, 137/390, d1=0.434, d2=0.171 g=2.725\n",
            ">1, 138/390, d1=0.411, d2=0.140 g=2.958\n",
            ">1, 139/390, d1=0.515, d2=0.218 g=2.847\n",
            ">1, 140/390, d1=0.573, d2=0.169 g=2.719\n",
            ">1, 141/390, d1=0.576, d2=0.149 g=2.684\n",
            ">1, 142/390, d1=0.408, d2=0.135 g=2.950\n",
            ">1, 143/390, d1=0.372, d2=0.178 g=2.897\n",
            ">1, 144/390, d1=0.610, d2=0.190 g=2.785\n",
            ">1, 145/390, d1=0.409, d2=0.114 g=3.029\n",
            ">1, 146/390, d1=0.621, d2=0.138 g=2.609\n",
            ">1, 147/390, d1=0.397, d2=0.172 g=2.826\n",
            ">1, 148/390, d1=0.373, d2=0.117 g=3.078\n",
            ">1, 149/390, d1=0.280, d2=0.134 g=3.255\n",
            ">1, 150/390, d1=0.347, d2=0.121 g=3.197\n",
            ">1, 151/390, d1=0.250, d2=0.096 g=3.268\n",
            ">1, 152/390, d1=0.323, d2=0.161 g=3.156\n",
            ">1, 153/390, d1=0.256, d2=0.143 g=3.482\n",
            ">1, 154/390, d1=0.326, d2=0.138 g=3.445\n",
            ">1, 155/390, d1=0.288, d2=0.087 g=3.357\n",
            ">1, 156/390, d1=0.309, d2=0.081 g=3.494\n",
            ">1, 157/390, d1=0.388, d2=0.077 g=3.098\n",
            ">1, 158/390, d1=0.225, d2=0.104 g=3.425\n",
            ">1, 159/390, d1=0.304, d2=0.070 g=3.216\n",
            ">1, 160/390, d1=0.200, d2=0.131 g=3.439\n",
            ">1, 161/390, d1=0.188, d2=0.120 g=3.514\n",
            ">1, 162/390, d1=0.208, d2=0.134 g=3.592\n",
            ">1, 163/390, d1=0.179, d2=0.182 g=3.315\n",
            ">1, 164/390, d1=0.261, d2=0.130 g=3.294\n",
            ">1, 165/390, d1=0.263, d2=0.111 g=3.519\n",
            ">1, 166/390, d1=0.212, d2=0.088 g=3.452\n",
            ">1, 167/390, d1=0.297, d2=0.183 g=3.391\n",
            ">1, 168/390, d1=0.210, d2=0.104 g=3.392\n",
            ">1, 169/390, d1=0.232, d2=0.164 g=3.398\n",
            ">1, 170/390, d1=0.166, d2=0.185 g=3.436\n",
            ">1, 171/390, d1=0.142, d2=0.101 g=3.465\n",
            ">1, 172/390, d1=0.173, d2=0.144 g=3.715\n",
            ">1, 173/390, d1=0.208, d2=0.177 g=3.452\n",
            ">1, 174/390, d1=0.154, d2=0.152 g=3.457\n",
            ">1, 175/390, d1=0.329, d2=0.184 g=3.350\n",
            ">1, 176/390, d1=0.187, d2=0.152 g=3.422\n",
            ">1, 177/390, d1=0.271, d2=0.291 g=3.254\n",
            ">1, 178/390, d1=0.172, d2=0.220 g=3.471\n",
            ">1, 179/390, d1=0.232, d2=0.346 g=3.314\n",
            ">1, 180/390, d1=0.100, d2=0.161 g=3.578\n",
            ">1, 181/390, d1=0.181, d2=0.051 g=3.426\n",
            ">1, 182/390, d1=0.256, d2=0.128 g=3.105\n",
            ">1, 183/390, d1=0.224, d2=0.119 g=3.158\n",
            ">1, 184/390, d1=0.183, d2=0.272 g=3.353\n",
            ">1, 185/390, d1=0.211, d2=0.157 g=3.487\n",
            ">1, 186/390, d1=0.186, d2=0.117 g=3.435\n",
            ">1, 187/390, d1=0.139, d2=0.124 g=3.483\n",
            ">1, 188/390, d1=0.154, d2=0.136 g=3.662\n",
            ">1, 189/390, d1=0.187, d2=0.178 g=3.578\n",
            ">1, 190/390, d1=0.164, d2=0.407 g=5.421\n",
            ">1, 191/390, d1=0.296, d2=0.104 g=6.210\n",
            ">1, 192/390, d1=0.448, d2=0.094 g=5.047\n",
            ">1, 193/390, d1=0.528, d2=0.184 g=3.059\n",
            ">1, 194/390, d1=0.200, d2=0.182 g=3.699\n",
            ">1, 195/390, d1=0.252, d2=0.087 g=4.021\n",
            ">1, 196/390, d1=0.217, d2=0.094 g=3.770\n",
            ">1, 197/390, d1=0.192, d2=0.079 g=3.765\n",
            ">1, 198/390, d1=0.120, d2=0.123 g=3.742\n",
            ">1, 199/390, d1=0.216, d2=0.166 g=3.974\n",
            ">1, 200/390, d1=0.097, d2=0.122 g=4.022\n",
            ">1, 201/390, d1=0.130, d2=0.150 g=4.365\n",
            ">1, 202/390, d1=0.116, d2=0.031 g=4.264\n",
            ">1, 203/390, d1=0.097, d2=0.104 g=4.609\n",
            ">1, 204/390, d1=0.119, d2=0.092 g=4.279\n",
            ">1, 205/390, d1=0.069, d2=0.105 g=4.847\n",
            ">1, 206/390, d1=0.084, d2=0.106 g=4.732\n",
            ">1, 207/390, d1=0.133, d2=0.046 g=4.912\n",
            ">1, 208/390, d1=0.096, d2=0.073 g=4.865\n",
            ">1, 209/390, d1=0.055, d2=0.098 g=5.174\n",
            ">1, 210/390, d1=0.109, d2=0.063 g=4.826\n",
            ">1, 211/390, d1=0.064, d2=0.530 g=7.581\n",
            ">1, 212/390, d1=0.344, d2=0.070 g=9.561\n",
            ">1, 213/390, d1=0.409, d2=0.061 g=9.324\n",
            ">1, 214/390, d1=0.304, d2=0.033 g=7.916\n",
            ">1, 215/390, d1=0.167, d2=0.074 g=6.668\n",
            ">1, 216/390, d1=0.125, d2=0.081 g=6.265\n",
            ">1, 217/390, d1=0.115, d2=0.098 g=5.898\n",
            ">1, 218/390, d1=0.092, d2=0.076 g=4.982\n",
            ">1, 219/390, d1=0.133, d2=0.318 g=3.601\n",
            ">1, 220/390, d1=0.119, d2=0.200 g=4.110\n",
            ">1, 221/390, d1=0.209, d2=0.060 g=4.471\n",
            ">1, 222/390, d1=0.441, d2=0.119 g=3.789\n",
            ">1, 223/390, d1=0.325, d2=0.121 g=3.507\n",
            ">1, 224/390, d1=0.222, d2=0.159 g=4.102\n",
            ">1, 225/390, d1=0.276, d2=0.107 g=3.806\n",
            ">1, 226/390, d1=0.140, d2=0.083 g=3.819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXo0vg0uyQLN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}